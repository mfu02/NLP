{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5927729",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snscrape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msntwitter\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'snscrape'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989e5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_usernames = ['lsxyz9', 'moleculelol', 'IWDominate',\n",
    "               'Caedrel', 'VeigarV2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348fd3ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sntwitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m lol_tweets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_name \u001b[38;5;129;01min\u001b[39;00m lol_usernames:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43msntwitter\u001b[49m\u001b[38;5;241m.\u001b[39mTwitterUserScraper(user_name)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[0;32m      5\u001b[0m         lol_tweets\u001b[38;5;241m.\u001b[39mappend(tweet) \n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m500\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sntwitter' is not defined"
     ]
    }
   ],
   "source": [
    "lol_tweets = []\n",
    "\n",
    "for user_name in lol_usernames:\n",
    "    for i, tweet in enumerate(sntwitter.TwitterUserScraper(user_name).get_items()):\n",
    "        lol_tweets.append(tweet) \n",
    "        if i > 500:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe04414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([tweet.__dict__ for tweet in lol_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb767e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91fd954",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'renderedContent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'renderedContent'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df2\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrenderedContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'renderedContent'"
     ]
    }
   ],
   "source": [
    "df2=df['renderedContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "            if self.verbose and i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'Loss: {self.__loss(h, y)} \\t')\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff771416",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_terms={\"league\",\n",
    "           \"lck\",\n",
    "           \"lpl\",\n",
    "           \"lcs\",\n",
    "           \"lec\",\n",
    "           \"aatrox\", \"ahri\", \"akali\", \"alistar\", \"amumu\", \"anivia\", \"annie\", \"aphelios\", \"ashe\", \"aurelion sol\", \"azir\", \"bard\", \"blitzcrank\", \"brand\", \"braum\", \"caitlyn\", \"camille\", \"cassiopeia\", \"cho'gath\", \"corki\", \"darius\", \"diana\", \"dr. mundo\", \"draven\", \"ekko\", \"elise\", \"evelynn\", \"ezreal\", \"fiddlesticks\", \"fiora\", \"fizz\", \"galio\", \"gangplank\", \"garen\", \"gnar\", \"gragas\", \"graves\", \"hecarim\", \"heimerdinger\", \"illaoi\", \"irelia\", \"ivern\", \"janna\", \"jarvan iv\", \"jax\", \"jayce\", \"jhin\", \"jinx\", \"kai'sa\", \"kalista\", \"karma\", \"karthus\", \"kassadin\", \"katarina\", \"kayle\", \"kayn\", \"kennen\", \"kha'zix\", \"kindred\", \"kled\", \"kog'maw\", \"leblanc\", \"lee sin\", \"leona\", \"lillia\", \"lissandra\", \"lucian\", \"lulu\", \"lux\", \"malphite\", \"malzahar\", \"maokai\", \"master yi\", \"miss fortune\", \"mordekaiser\", \"morgana\", \"nami\", \"nasus\", \"nautilus\", \"neeko\", \"nidalee\", \"nocturne\", \"nunu & willump\", \"olaf\", \"orianna\", \"ornn\", \"pantheon\", \"poppy\", \"pyke\", \"qiyana\", \"quinn\", \"rakan\", \"rammus\", \"rek'sai\", \"renekton\", \"rengar\", \"riven\", \"rumble\", \"ryze\", \"samira\", \"sejuani\", \"senna\", \"seraphine\", \"sett\", \"shaco\", \"shen\", \"shyvana\", \"singed\", \"sion\", \"sivir\", \"skarner\", \"sona\", \"soraka\", \"swain\", \"sylas\", \"syndra\", \"tahm kench\", \"taliyah\", \"talon\", \"taric\", \"teemo\", \"thresh\", \"tristana\", \"trundle\", \"tryndamere\", \"twisted fate\", \"twitch\", \"udyr\", \"urgot\", \"varus\", \"vayne\", \"veigar\", \"vel'koz\", \"vi\", \"viktor\", \"vladimir\", \"volibear\", \"warwick\", \"wukong\", \"xayah\", \"xerath\", \"xin zhao\", \"yasuo\", \"yone\", \"yorick\", \"yuumi\", \"zac\", \"zed\", \"ziggs\", \"zilean\", \"zoe\", \"zyra\", \"vex\", \"k'sante\", \"millio\", \"bel'veth\", \"renata\", \"zeri\", \"nilah\", \"akshan\", \"gwen\", \"viego\", \"rell\",\n",
    "           \"fnatic\", \"g2 esports\", \"mad lions\", \"rogue\", \"excel esports\", \"koi\", \"sk gaming\", \"team vitality\", \"astralis\", \"bds esports\",\"fnc\", \"g2\", \"mad\", \"rge\", \"xl\", \"koi\", \"sk\", \"vit\", \"ast\", \"bds\",\n",
    "           \"100\", \"c9\", \"clg\", \"dig\", \"eg\", \"fly\", \"ggs\", \"imt\", \"tl\", \"tsm\", \"100 thieves\", \"cloud9\", \"counter logic gaming\", \"dignitas\", \"evil geniuses\", \"flyquest\", \"golden guardians\", \"immortals\", \"team liquid\", \"team solomid\",\n",
    "           \"t1\", \"gen.g\", \"drx\", \"dplus\", \"afreeca freecs\", \"kt rolster\", \"liiv sandbox\", \"hanwha life esports\", \"nongshim redforce\", \"brion\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_keyword(text, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['filtered'] = df2['renderedContent'].progress_apply(lambda x: \n",
    "                                             [x.text.lower() for x in NLP(x) if x.text.lower() not in undesired]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db232a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df2[df2['filtered'].apply(lambda x: contains_keyword(x, lol_terms))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc024d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "s=filtered_df.apply(lambda x: sentiment_pipeline(x))\n",
    "filterd_df['sentiment']=pd.DataFrame([x[0] for x in df2])['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d8a2572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23255052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae4df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d471384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d83cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b47ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "NLP = spacy.load(\"en_core_web_sm\")\n",
    "def extract_noun_adj(text):\n",
    "    doc = NLP(text)\n",
    "    noun_adj = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"ADJ\"]:\n",
    "            noun_adj.append(token.text)\n",
    "    return noun_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c50449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "filtered_df['noun_adjective'] = filtered_df.progress_apply(lambda x: extract_noun_adj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33035402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'positive': 1, 'negative': 0}\n",
    "filtered_df['sentiment'] = df['sentiment'].map(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noun_adjective_string'] = df['noun_adjective'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_vectorizer = CountVectorizer()\n",
    "noun_X = noun_vectorizer.fit_transform(df['noun_adjective_string'].values.astype('U')).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_Y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_X_train, noun_X_val, noun_Y_train, noun_Y_val = train_test_split(noun_X, noun_Y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af057289",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_reg = LogisticRegression(lr=0.01, num_iter=1000, fit_intercept=True, verbose=True)\n",
    "noun_reg.fit(noun_X_train, noun_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_Y_pred = noun_reg.predict(noun_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5196c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(noun_Y_val, noun_Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cfm = confusion_matrix(noun_Y_val, noun_Y_pred)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(cfm, annot=True, fmt='d', cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(noun_Y_val, noun_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34d297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203408ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13a19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cfdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
